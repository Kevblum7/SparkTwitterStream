{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb492070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import tweepy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "#consumer_key = \"Ax79DI6gXdMjOCQyQ61v2Khbj\"\n",
    "#consumer_secret = \"DBSmfUL50FlhpVnePa373lOpBFffJjrGInXbSGG4T7Ze2RRQTl\"\n",
    "#access_token = \"1418173405988376579-9g3SF5klnWOgd62nUfTfaqrQdzkJRA\"\n",
    "#access_token_secret = \"q3jO28xbGDlvE6djCNeoSMQbtkosZYT7mBTv0O6wznMpw\"\n",
    "# Creating the authentication object\n",
    "#auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "#auth.set_access_token(access_token, access_token_secret)\n",
    "#api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cb0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open 2 command prompts\n",
    "#terminal 1\n",
    "#cd %SPARK_HOME%\n",
    "#bin\\spark-class2.cmd org.apache.spark.deploy.master.Master\n",
    "#open in browser localhost:8080\n",
    "#terminal 2\n",
    "#cd %SPARK_HOME%\n",
    "#bin\\spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 4 -m 32G spark://192.168.1.227:7077\n",
    "#open in browser localhost:8081\n",
    "spark = SparkSession.builder.master('spark://192.168.1.227:7077').appName('classifier').config('spark.executor.memory', '32g').getOrCreate()\n",
    "df = spark.read.options(delimiter=\",\",header=False).csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\")\n",
    "df=df.toDF('polarity','tweet_id','tweet_date','query_string','tweet_screen_name','Tweet')\n",
    "df2=spark.read.options(delimiter=\",\",header=False).csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\")\n",
    "df2=df2.toDF('polarity','tweet_id','tweet_date','query_string','tweet_screen_name','Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175350c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "def preprocessing(text):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = text.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+',' ',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "tem=df.union(df2)\n",
    "brand_udf=udf(preprocessing,StringType())\n",
    "tem=tem.withColumn('text',brand_udf(tem['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac14afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "tem1=tem.select('tweet_id','Tweet','text','polarity')\n",
    "tem1 = tem1.withColumn('polarity', tem1['polarity'].cast(IntegerType()))\n",
    "(trainingData, testData) = tem1.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50126d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(minDocFreq=2, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(labelCol='polarity')\n",
    "\n",
    "# Pipeline Architecture\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc10fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "predictions1=predictions.toPandas()\n",
    "# Select example rows to display.\n",
    "#predictions.select(\"text\", \"polarity\", \"prediction\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1de0369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7529373419288519\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"polarity\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab672ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "testData=testData.toPandas()\n",
    "predictions=predictions1\n",
    "testData['prediction']=predictions['prediction']\n",
    "testData=testData[['tweet_id','Tweet','polarity','prediction']]\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ba41a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x2120890ae80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"tweet\"]\n",
    "records = json.loads(testData.T.to_json()).values()\n",
    "mydb.mycol.insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "368d8945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_ce548fa25a83\n"
     ]
    }
   ],
   "source": [
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475ad7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (CS644)",
   "language": "python",
   "name": "pycharm-9bc77049"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
