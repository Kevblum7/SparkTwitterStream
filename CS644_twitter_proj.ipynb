{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb492070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "consumer_key = \"Ax79DI6gXdMjOCQyQ61v2Khbj\"\n",
    "consumer_secret = \"DBSmfUL50FlhpVnePa373lOpBFffJjrGInXbSGG4T7Ze2RRQTl\"\n",
    "access_token = \"1418173405988376579-9g3SF5klnWOgd62nUfTfaqrQdzkJRA\"\n",
    "access_token_secret = \"q3jO28xbGDlvE6djCNeoSMQbtkosZYT7mBTv0O6wznMpw\"\n",
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cb0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------------+-----------------+--------------------+\n",
      "|polarity|  tweet_id|          tweet_date|query_string|tweet_screen_name|               Tweet|\n",
      "+--------+----------+--------------------+------------+-----------------+--------------------+\n",
      "|       0|1467810369|Mon Apr 06 22:19:...|    NO_QUERY|  _TheSpecialOne_|@switchfoot http:...|\n",
      "|       0|1467810672|Mon Apr 06 22:19:...|    NO_QUERY|    scotthamilton|is upset that he ...|\n",
      "|       0|1467810917|Mon Apr 06 22:19:...|    NO_QUERY|         mattycus|@Kenichan I dived...|\n",
      "|       0|1467811184|Mon Apr 06 22:19:...|    NO_QUERY|          ElleCTF|my whole body fee...|\n",
      "|       0|1467811193|Mon Apr 06 22:19:...|    NO_QUERY|           Karoli|@nationwideclass ...|\n",
      "+--------+----------+--------------------+------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+--------+--------------------+------------+-----------------+--------------------+\n",
      "|polarity|tweet_id|          tweet_date|query_string|tweet_screen_name|               Tweet|\n",
      "+--------+--------+--------------------+------------+-----------------+--------------------+\n",
      "|       4|       3|Mon May 11 03:17:...|     kindle2|           tpryan|@stellargirl I lo...|\n",
      "|       4|       4|Mon May 11 03:18:...|     kindle2|           vcu451|Reading my kindle...|\n",
      "|       4|       5|Mon May 11 03:18:...|     kindle2|           chadfu|Ok, first assesme...|\n",
      "|       4|       6|Mon May 11 03:19:...|     kindle2|            SIX15|@kenburbary You'l...|\n",
      "|       4|       7|Mon May 11 03:21:...|     kindle2|         yamarama|@mikefish  Fair e...|\n",
      "+--------+--------+--------------------+------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('classifier').getOrCreate()\n",
    "df = spark.read.csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\")\n",
    "df=df.toDF('polarity','tweet_id','tweet_date','query_string','tweet_screen_name','Tweet')\n",
    "df2=spark.read.csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\")\n",
    "df2=df2.toDF('polarity','tweet_id','tweet_date','query_string','tweet_screen_name','Tweet')\n",
    "df.show(5)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03752022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|polarity|count |\n",
      "+--------+------+\n",
      "|0       |800000|\n",
      "|4       |800000|\n",
      "+--------+------+\n",
      "\n",
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|4       |182  |\n",
      "|0       |177  |\n",
      "|2       |139  |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df[['tweet_id','query_string','Tweet','polarity']]\n",
    "df2=df2[['tweet_id','query_string','Tweet','polarity']]\n",
    "df.groupBy('polarity').count().orderBy('count',ascending=False).show(10,False)\n",
    "df2.groupBy('polarity').count().orderBy('count',ascending=False).show(10,False)\n",
    "#df=df.toPandas()\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175350c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import *\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyspark.sql.types import StringType,DoubleType,IntegerType\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "worddict = set(nltk.corpus.words.words())\n",
    "\n",
    "def preprocessing(text):\n",
    "    wordset_n = set(wn.lemmatize(w, NOUN) for w in word_tokenize(text.lower().strip()))\n",
    "    wordset_v = set(wn.lemmatize(w, VERB) for w in wordset_n)\n",
    "    wordset = set(wn.lemmatize(w, ADJ) for w in wordset_v)\n",
    "    wordset = wordset & worddict\n",
    "    return ' '.join(list(wordset))\n",
    "brand_udf=udf(preprocessing,StringType())\n",
    "df=df.withColumn('text',brand_udf(df['Tweet']))\n",
    "df=df.withColumn('clean_len',F.length('text'))\n",
    "df2=df2.withColumn('text',brand_udf(df2['Tweet']))\n",
    "df2=df2.withColumn('clean_len',F.length('text'))\n",
    "#tem=df.union(df2)\n",
    "#tem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca4f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import  Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#tem = tokenizer.transform(tem)\n",
    "df=tokenizer.transform(df)\n",
    "df2=tokenizer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6414ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "count = CountVectorizer (inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "#model = count.fit(tem)\n",
    "#tem = model.transform(tem)\n",
    "#tem.show()\n",
    "model1=count.fit(df)\n",
    "df=model1.transform(df)\n",
    "model2=count.fit(df2)\n",
    "df2=model2.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56d75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import  IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(tem)\n",
    "#tem = idfModel.transform(tem)\n",
    "#tem = tem.withColumn('polarity', tem['polarity'].cast(IntegerType()))\n",
    "idfModel = idf.fit(df)\n",
    "df = idfModel.transform(df)\n",
    "df = df.withColumn('polarity', df['polarity'].cast(IntegerType()))\n",
    "idfModel = idf.fit(df2)\n",
    "df2 = idfModel.transform(df2)\n",
    "df2 = df2.withColumn('polarity', df2['polarity'].cast(IntegerType()))\n",
    "trainingDF,testDF = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef88294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf_classifier=RandomForestClassifier(labelCol='polarity',numTrees=50).fit(trainingDF.select(['text','words','rawFeatures','features','polarity']))\n",
    "rf_predictions=rf_classifier.transform(testDF.select(['text','words','rawFeatures','features','polarity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7db99b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o460.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 167) (192.168.1.227 executor driver): java.io.IOException: Failed to delete original file 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\spark-8935d2c5-a375-429e-9094-5b244d1df78f\\broadcast5379028567372557823' after copy to 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\blockmgr-c1994c25-a717-4fdc-976b-25b2e92cb905\\08\\broadcast_16_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2276)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:133)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:478)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:397)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1408)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:370)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:481)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:783)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:785)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:772)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1627)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:935)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:231)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$20(PythonRunner.scala:415)\r\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:411)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:259)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Failed to delete original file 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\spark-8935d2c5-a375-429e-9094-5b244d1df78f\\broadcast5379028567372557823' after copy to 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\blockmgr-c1994c25-a717-4fdc-976b-25b2e92cb905\\08\\broadcast_16_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2276)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:133)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:478)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:397)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1408)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:370)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:481)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:783)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:785)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:772)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1627)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:935)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:231)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$20(PythonRunner.scala:415)\r\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:411)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:259)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SAJIN~1.LAP\\AppData\\Local\\Temp/ipykernel_16204/3644069626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrf_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\CS644\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS644\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS644\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS644\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o460.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 167) (192.168.1.227 executor driver): java.io.IOException: Failed to delete original file 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\spark-8935d2c5-a375-429e-9094-5b244d1df78f\\broadcast5379028567372557823' after copy to 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\blockmgr-c1994c25-a717-4fdc-976b-25b2e92cb905\\08\\broadcast_16_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2276)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:133)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:478)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:397)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1408)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:370)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:481)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:783)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:785)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:772)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1627)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:935)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:231)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$20(PythonRunner.scala:415)\r\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:411)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:259)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Failed to delete original file 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\spark-8935d2c5-a375-429e-9094-5b244d1df78f\\broadcast5379028567372557823' after copy to 'C:\\Users\\Sajin.LAPTOP-RE0DL8PH\\AppData\\Local\\Temp\\blockmgr-c1994c25-a717-4fdc-976b-25b2e92cb905\\08\\broadcast_16_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2276)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:133)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:478)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:397)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1408)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:370)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:481)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:783)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:785)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:772)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1627)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:935)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:231)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1428)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:226)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$20(PythonRunner.scala:415)\r\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:411)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:259)\r\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow every follower of the authenticated user\n",
    "timeline = api.home_timeline()\n",
    "data = pd.DataFrame(data=[tweet.text for tweet in timeline], columns=['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c607311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (CS644)",
   "language": "python",
   "name": "pycharm-9bc77049"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
